{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d607f0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c78706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from datasets.inria import InriaDataset\n",
    "from datasets.poland import PolandDataset\n",
    "from loss_functions.tversky_focal_loss import TverskyFocalLoss\n",
    "from segmentation_models_pytorch import Unet\n",
    "from torchmetrics import Accuracy, JaccardIndex, F1Score\n",
    "import mlflow\n",
    "import logging\n",
    "import tqdm\n",
    "import json\n",
    "import os\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c43d27",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f8c65",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739351d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: torch.nn.Module, train_loader: torch.utils.data.DataLoader, epoch: int, epochs: int,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                criterion: torch.nn.Module, scheduler: torch.optim.lr_scheduler.LRScheduler, \n",
    "                warmup_epochs:int, accumulation_steps: int,\n",
    "                device: str, scaler: torch.amp.grad_scaler.GradScaler):\n",
    "    \"\"\"Executes a single epoch. Uses iteration-wise warmup scheduler, gradient accumulation and\n",
    "    mixed precision training.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        train_loader (torch.utils.data.DataLoader): Train dataloader.\n",
    "        epoch (int): Current epoch.\n",
    "        epochs (int): Total num of epochs.\n",
    "        optimizer (torch.optim.Optimizer): Loss optimizer.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        scheduler (torch.optim.lr_scheduler.LRScheduler): LR warmup scheduler (warmup).\n",
    "        warmup_epochs (int): Num of epochs to use warmup scheduler.\n",
    "        accumulation_steps (int): Amount of forward passes taken before single backward.\n",
    "        device (str): CPU / GPU.\n",
    "        scaler (torch.amp.grad_scaler.GradScaler): Gradient scaler.\n",
    "\n",
    "    Returns:\n",
    "        dict: Epoch metrics.\n",
    "    \"\"\"    \n",
    " \n",
    "    processed_data = 0\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_dice = 0.0\n",
    "    accuracy = Accuracy(task='binary', threshold=0.5, average='weighted', ignore_index=255).to(device)\n",
    "    iou = JaccardIndex(task='binary', threshold=0.5, average='weighted', ignore_index=255).to(device)\n",
    "    dice = F1Score(task='binary', threshold=0.5, average='weighted', ignore_index=255).to(device)\n",
    "    \n",
    "    loop_train = tqdm.tqdm(enumerate(train_loader), total=len(train_loader), leave=False,\n",
    "                            desc=f'Epoch[{epoch+1}/{epochs}] train')\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for iteration, (inputs, targets) in loop_train:\n",
    "        inputs = inputs.to(device, non_blocking=True) \n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        # use mixed precision\n",
    "        with torch.amp.autocast(device_type=device):\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "            # scale loss according to acc. steps\n",
    "            loss = criterion(outputs, targets) / accumulation_steps\n",
    "\n",
    "        # accumulate loss\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # propagate loss each n-th iteration or last iteration of epoch\n",
    "        if (iteration + 1) % accumulation_steps == 0 or iteration + 1 == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # make warmup scheduler step\n",
    "            if epoch < warmup_epochs and scheduler is not None:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # compute metrics\n",
    "        inputs = inputs.detach()\n",
    "        outputs = outputs.detach()\n",
    "        targets = targets.detach()\n",
    "        processed_data += inputs.size(0)\n",
    "\n",
    "        batch_loss = loss.item() * accumulation_steps * inputs.size(0)\n",
    "        batch_accuracy = accuracy(outputs.squeeze(), targets).item()\n",
    "        batch_iou = iou(outputs.squeeze(), targets).item()\n",
    "        batch_dice = dice(outputs.squeeze(), targets).item()\n",
    "        \n",
    "        running_loss += batch_loss\n",
    "        running_accuracy += batch_accuracy * inputs.size(0)\n",
    "        running_iou += batch_iou * inputs.size(0)\n",
    "        running_dice += batch_dice * inputs.size(0)\n",
    "\n",
    "        # log metrics each 100-th iteration\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            mlflow.log_metrics({'train_batch_loss': batch_loss,\n",
    "                                'train_loss_propagated': loss.item(),\n",
    "                                'train_batch_accuracy': batch_accuracy,\n",
    "                                'train_batch_iou': batch_iou,\n",
    "                                'train_batch_dice': batch_dice,}, \n",
    "                                 step=iteration + 1)  \n",
    "        \n",
    "        loop_train.set_postfix(batch=iteration+1, loss=loss.item(), iou=batch_iou)   \n",
    "        \n",
    "    return {'train_avg_loss': running_loss / processed_data,\n",
    "            'train_avg_accuracy': running_accuracy / processed_data,\n",
    "            'train_avg_iou_score': running_iou / processed_data,\n",
    "            'train_avg_dice_score': running_dice / processed_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f74e",
   "metadata": {},
   "source": [
    "## Val loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model: torch.nn.Module, val_loader: torch.utils.data.DataLoader, \n",
    "              epoch: int, epochs: int, criterion: torch.nn.Module, device: str):\n",
    "    \n",
    "    \"\"\"Executes a single validation epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation dataloader.\n",
    "        epoch (int): Current epoch.\n",
    "        epochs (int): Total num of epochs.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (str): CPU / GPU.\n",
    "\n",
    "    Returns:\n",
    "        dict: Epoch metrics.\n",
    "    \"\"\"     \n",
    "    \n",
    "    processed_data = 0\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_dice = 0.0\n",
    "    accuracy = Accuracy(task='binary', threshold=0.5, average='weighted', ignore_index=255).to(device)\n",
    "    iou = JaccardIndex(task='binary', threshold=0.5, average='weighted', ignore_index=255).to(device)\n",
    "    dice = F1Score(task='binary', threshold=0.5, average='weighted', ignore_index=255).to(device)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loop_val = tqdm.tqdm(enumerate(val_loader), total=len(val_loader), leave=False,\n",
    "                                desc=f'Epoch[{epoch+1}/{epochs}] val') \n",
    "        \n",
    "        for iteration, (inputs, targets) in loop_val:\n",
    "            inputs = inputs.to(device, non_blocking=True) \n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            inputs = inputs.detach()\n",
    "            outputs = outputs.detach()\n",
    "            targets = targets.detach()\n",
    "            processed_data += inputs.size(0)\n",
    "\n",
    "            batch_loss = loss.item() * inputs.size(0)\n",
    "            batch_accuracy = accuracy(outputs.squeeze(), targets).item()\n",
    "            batch_iou = iou(outputs.squeeze(), targets).item()\n",
    "            batch_dice = dice(outputs.squeeze(), targets).item()\n",
    "            \n",
    "            running_loss += batch_loss\n",
    "            running_accuracy += batch_accuracy * inputs.size(0)\n",
    "            running_iou += batch_iou * inputs.size(0)\n",
    "            running_dice += batch_dice * inputs.size(0)\n",
    "            \n",
    "            # log metrics and examples of inference\n",
    "            if iteration % 100 == 0:\n",
    "                mlflow.log_metrics({'val_batch_loss': batch_loss,\n",
    "                                    'val_loss_propagated': loss.item(),\n",
    "                                    'val_batch_accuracy': batch_accuracy,\n",
    "                                    'val_batch_iou': batch_iou,\n",
    "                                    'val_batch_dice': batch_dice,}, \n",
    "                                    step=iteration + 1)  \n",
    "                try:\n",
    "                    mlflow.active_run().info.run_name\n",
    "                    example = train_utils.make_example(inputs, targets, outputs)\n",
    "                    mlflow.log_image(example, artifact_file=f'example_{iteration+1}.png')\n",
    "                except Exception as e:\n",
    "                    with open('artifacts\\\\log.txt', 'a') as f:\n",
    "                        f.write(str(f'{e}\\n'))\n",
    "            \n",
    "            loop_val.set_postfix(batch=iteration+1, loss=loss.item(), iou=batch_iou)\n",
    "           \n",
    "    return {'val_avg_loss': running_loss / processed_data,\n",
    "            'val_avg_accuracy': running_accuracy / processed_data,\n",
    "            'val_avg_iou_score': running_iou / processed_data,\n",
    "            'val_avg_dice_score': running_dice / processed_data} \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de94efc",
   "metadata": {},
   "source": [
    "## Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(run_id: str, model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "        epochs: int, optimizer: torch.optim.Optimizer, patience: int, train_loader: torch.utils.data.DataLoader, \n",
    "        val_loader: torch.utils.data.DataLoader, scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "        warmup_scheduler: torch.optim.lr_scheduler.LRScheduler, \n",
    "        warmup_epochs: int, acc_steps: int, device: str):\n",
    "    \"\"\"Performs training of a model. Uses task-specific segmentation metrics.\n",
    "\n",
    "    Args:\n",
    "        run_id (str): MLflow run id.\n",
    "        model (torch.nn.Module): Model.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        epochs (int): Total num of epochs.\n",
    "        optimizer (torch.optim.Optimizer): Loss optimizer.\n",
    "        patience (int): Num of epochs with no improvement to halt the training.\n",
    "        train_loader (torch.utils.data.DataLoader): Training dataloader.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation dataloader.\n",
    "        scheduler (torch.optim.lr_scheduler.LRScheduler): LR scheduler.\n",
    "        warmup_scheduler (torch.optim.lr_scheduler.LRScheduler): Warmup LR scheduler.\n",
    "        warmup_epochs (int): Num of epochs to use warmup scheduler.\n",
    "        acc_steps (int): Amount of forward passes taken before single backward.\n",
    "        device (str): CPU / GPU.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training and validation metrics.\n",
    "    \"\"\"      \n",
    "    parent_run_name = mlflow.get_run(run_id).info.run_name\n",
    "    \n",
    "    mlflow.set_tag('status', 'training')\n",
    "    best_dice = float('-inf')\n",
    "    best_iou = float('-inf')\n",
    "    counter = 0\n",
    "\n",
    "    # initialize metric lists\n",
    "    train_losses = []\n",
    "    train_accuracy_scores = []\n",
    "    train_iou_scores = []\n",
    "    train_dice_scores = []\n",
    "    \n",
    "    val_losses = []\n",
    "    val_accuracy_scores = []\n",
    "    val_iou_scores = []\n",
    "    val_dice_scores = []\n",
    "\n",
    "    \n",
    "    scaler = torch.amp.GradScaler()\n",
    "    # training\n",
    "    for epoch in range(epochs):\n",
    "        # train loop and lr logging\n",
    "        mlflow.log_metrics({f'group_{i}_lr': lr for (i, lr) in enumerate(scheduler.get_last_lr())}, step=epoch+1, run_id=run_id)\n",
    "        mlflow.start_run(run_name=f'Epoch_{epoch+1}', nested=True, parent_run_id=run_id)\n",
    "        train_results = train_epoch(model, train_loader, epoch, epochs, optimizer, criterion, \n",
    "                                    warmup_scheduler, warmup_epochs, acc_steps, device, scaler)\n",
    "        # save checkpoint \n",
    "        checkpoint_path = f'artifacts\\\\{parent_run_name}\\\\checkpoints\\\\epoch{epoch+1}'\n",
    "        train_utils.save_checkpoint(epoch, checkpoint_path, model, optimizer, scheduler, scaler, run_id=run_id)\n",
    "        \n",
    "        # refresh lrs \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_results['train_avg_loss'])\n",
    "        elif scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # log and update metrics\n",
    "        mlflow.log_metrics(train_results, step=epoch+1, run_id=run_id)  \n",
    "        train_losses.append(train_results['train_avg_loss'])\n",
    "        train_accuracy_scores.append(train_results['train_avg_accuracy'])\n",
    "        train_iou_scores.append(train_results['train_avg_iou_score'])\n",
    "        train_dice_scores.append(train_results['train_avg_dice_score'])\n",
    "        \n",
    "        # val loop\n",
    "        val_results = val_epoch(model, val_loader, epoch, epochs, criterion, device)\n",
    "  \n",
    "        mlflow.log_metrics(val_results, step=epoch+1, run_id=run_id)\n",
    "        val_losses.append(val_results['val_avg_loss'])\n",
    "        val_accuracy_scores.append(val_results['val_avg_accuracy'])\n",
    "        val_iou_scores.append(val_results['val_avg_iou_score'])\n",
    "        val_dice_scores.append(val_results['val_avg_dice_score'])\n",
    "        \n",
    "\n",
    "        # early stopping\n",
    "        indexing_range = patience if patience <= (epoch + 1) else (epoch + 1)\n",
    "        \n",
    "        save_condition = val_dice_scores[-1] > best_dice or val_iou_scores[-1] > best_iou\n",
    "        \n",
    "        if epoch > 0:\n",
    "            val_dice_array = np.array(val_dice_scores)\n",
    "            val_iou_array = np.array(val_iou_scores)\n",
    "            plateau_condition = (np.abs(np.diff(val_dice_array[-indexing_range:])).mean() < 0.01 and \\\n",
    "            np.abs(np.diff(val_iou_array[-indexing_range:])).mean() < 0.01)\n",
    "        else:\n",
    "            plateau_condition = False\n",
    "\n",
    "        if save_condition:\n",
    "            counter = 0\n",
    "            best_dice = max(best_dice, val_dice_scores[-1])\n",
    "            best_iou = max(best_iou, val_iou_scores[-1])     \n",
    "            mlflow.log_metric('best_epoch', epoch+1, step=epoch+1, run_id=run_id)\n",
    "        elif plateau_condition:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                mlflow.set_tag('status', 'early stopping because of plateau')\n",
    "                break\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter > patience:\n",
    "                mlflow.set_tag('status', 'early stopping')\n",
    "                break   \n",
    "        mlflow.end_run()\n",
    "        \n",
    "    mlflow.pytorch.log_model(model)\n",
    "    res = {'train_losses': train_losses,\n",
    "           'train_accuracy_scores': train_accuracy_scores,\n",
    "           'train_iou_scores': train_iou_scores,\n",
    "           'train_dice_scores': train_dice_scores,\n",
    "           'val_losses': val_losses,\n",
    "           'val_accuracy_scores': val_accuracy_scores,\n",
    "           'val_iou_scores': val_iou_scores,\n",
    "           'val_dice_scores': val_dice_scores}\n",
    "           \n",
    "    return res           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36c7f9",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "os.environ['MLFLOW_SUPPRESS_PRINTING_URL_TO_STDOUT'] = '1'\n",
    "experiment = mlflow.set_experiment('test')\n",
    "parent_run = mlflow.start_run(run_name='test')\n",
    "parent_id = parent_run.info.run_id\n",
    "parent_name = parent_run.info.run_name\n",
    "os.makedirs(f'artifacts\\\\{parent_name}\\\\checkpoints')\n",
    "mlflow.set_tag('status', 'setting up')\n",
    "mlflow_logger = logging.getLogger('mlflow')\n",
    "mlflow_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a40e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "accumulation_steps = 2\n",
    "warmup_epochs = 3\n",
    "num_epochs = 20\n",
    "patience = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs = A.Compose([A.OneOf([\n",
    "                            A.Resize(256, 256, p=0.2),\n",
    "                            A.Compose([\n",
    "                                A.OneOf([\n",
    "                                    A.Resize(384, 384, p=0.625),\n",
    "                                    A.Resize(512, 512, p=0.375)\n",
    "                                ], p=1.0),\n",
    "                                A.ShiftScaleRotate(\n",
    "                                    shift_limit=0, scale_limit=(-0.2, 0.2), rotate_limit=0,\n",
    "                                    fill=255, fill_mask=255, p=0.6),\n",
    "                                A.ShiftScaleRotate(\n",
    "                                    shift_limit=0, scale_limit=0, rotate_limit=(-30, 30),\n",
    "                                    fill=255, fill_mask=255, p=0.6)\n",
    "                            ], p=0.8)], p=1.0),\n",
    "\n",
    "                        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0, p=0.4),\n",
    "                        A.RandomGamma((85, 115), p=0.4),\n",
    "                        A.GaussianBlur((3, 4), (0.3, 0.8), p=0.4),\n",
    "                        A.GaussNoise((0, 0.1), (0, 0.05), p=0.3),\n",
    "\n",
    "                        A.PadIfNeeded(min_height=512,\n",
    "                                    min_width=512,\n",
    "                                    fill=255,\n",
    "                                    fill_mask=255),\n",
    "                        # A.Normalize(mean=(0.485, 0.456, 0.406),   IMAGENET STATS\n",
    "                        #         std=(0.229, 0.224, 0.225)),\n",
    "                \n",
    "                        A.Normalize(mean=(0.4014, 0.4235, 0.3888), #INRIA STATS\n",
    "                                std=(0.1708, 0.1555, 0.1457)),\n",
    "                        A.ToTensorV2()], additional_targets={'mask': 'mask'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ef108",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_augs = A.Compose([A.Resize(384, 384),\n",
    "                        A.PadIfNeeded(min_height=512,\n",
    "                                min_width=512,\n",
    "                                fill=255,\n",
    "                                fill_mask=255),\n",
    "                        A.Normalize(mean=(0.4014, 0.4235, 0.3888), #INRIA STATS\n",
    "                                std=(0.1708, 0.1555, 0.1457)),\n",
    "                        A.ToTensorV2()], additional_targets={'mask': 'mask'})                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44312a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and log datasets\n",
    "\n",
    "idx_a = np.random.permutation(180)[:120] \n",
    "np.random.shuffle(idx_a)\n",
    "split = int(len(idx_a) * 0.8)\n",
    "train_inria_idx, val_inria_idx = idx_a[:split], idx_a[split:]\n",
    "np.save(f'artifacts\\\\{parent_name}\\\\train_inria_idx.npy', train_inria_idx)\n",
    "np.save(f'artifacts\\\\{parent_name}\\\\val_inria_idx.npy', val_inria_idx)\n",
    "mlflow.log_artifact(f'artifacts\\\\{parent_name}\\\\train_inria_idx.npy')\n",
    "mlflow.log_artifact(f'artifacts\\\\{parent_name}\\\\val_inria_idx.npy')\n",
    "inria_train_dataset = InriaDataset(mode='train', idx=train_inria_idx, res=512,\n",
    "                             overlap=0.33, transforms = train_augs)\n",
    "inria_val_dataset = InriaDataset(mode='val', idx=val_inria_idx, res=512, \n",
    "                                 overlap=0.0, transforms =val_augs)\n",
    "\n",
    "idx_b = np.random.permutation(10674)[:3250]\n",
    "np.random.shuffle(idx_b)\n",
    "split = int(len(idx_b) * 0.8)\n",
    "train_poland_idx, val_poland_idx = idx_b[:split], idx_b[split:]\n",
    "np.save(f'artifacts\\\\{parent_name}\\\\train_poland_idx.npy', train_poland_idx)\n",
    "np.save(f'artifacts\\\\{parent_name}\\\\val_poland_idx.npy', val_poland_idx)\n",
    "mlflow.log_artifact(f'artifacts\\\\{parent_name}\\\\train_poland_idx.npy')\n",
    "mlflow.log_artifact(f'artifacts\\\\{parent_name}\\\\val_poland_idx.npy')\n",
    "poland_train_dataset = PolandDataset(idx=train_poland_idx, transforms=train_augs)\n",
    "poland_val_dataset = PolandDataset(idx=val_poland_idx, transforms=val_augs)\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset([inria_train_dataset, poland_train_dataset])\n",
    "val_dataset = torch.utils.data.ConcatDataset([inria_val_dataset, poland_val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True, pin_memory=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Unet(encoder_name='resnet50', encoder_weights='imagenet').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = TverskyFocalLoss('binary', alpha_tversky=0.7, beta_tversky=0.3,\n",
    "                             alpha_focal=0.7, gamma_focal=1.5, ignore_index=255,\n",
    "                             smoothing=0.8, weight_focal=0.4, weight_tversky=0.6).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = [model.encoder.layer1, model.encoder.layer2, model.encoder.layer3, model.encoder.layer4, \n",
    "           model.decoder, model.segmentation_head]\n",
    "\n",
    "separated_params = {name: train_utils.separate_norms(module) for(name, module) in \n",
    "                    zip(('layer1', 'layer2', 'layer3', 'layer4', 'decoder', 'head'), modules)}\n",
    "\n",
    "opt_params = [{\"params\": separated_params['layer1'][0], \"weight_decay\": 1e-4, 'lr': 1e-5},\n",
    "              {\"params\": separated_params['layer1'][1], \"weight_decay\": 0.0, 'lr': 1e-5},\n",
    "              {\"params\": separated_params['layer2'][0], \"weight_decay\": 1e-4, 'lr': 3e-5},\n",
    "              {\"params\": separated_params['layer2'][1], \"weight_decay\": 0.0, 'lr': 3e-5},\n",
    "              {\"params\": separated_params['layer3'][0], \"weight_decay\": 1e-4, 'lr': 1e-4},\n",
    "              {\"params\": separated_params['layer3'][1], \"weight_decay\": 0.0, 'lr': 1e-4},\n",
    "              {\"params\": separated_params['layer4'][0], \"weight_decay\": 1e-4, 'lr': 3e-4},\n",
    "              {\"params\": separated_params['layer4'][1], \"weight_decay\": 0.0, 'lr': 3e-4},\n",
    "              {\"params\": separated_params['decoder'][0], \"weight_decay\": 1e-4, 'lr': 3e-4},\n",
    "              {\"params\": separated_params['decoder'][1], \"weight_decay\": 0.0, 'lr': 3e-4},\n",
    "              {\"params\": separated_params['head'][0], \"weight_decay\": 1e-4, 'lr': 3e-4},\n",
    "              {\"params\": separated_params['head'][1], \"weight_decay\": 0.0, 'lr': 3e-4}]\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=opt_params, lr=3e-4, foreach=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer=optimizer,\n",
    "                                                     start_factor=0.1,\n",
    "                                                     end_factor=1.0,\n",
    "                                                     total_iters=train_utils\n",
    "                                                     .count_warmup_iters(batch_size, accumulation_steps, warmup_epochs, num_epochs))  \n",
    "\n",
    "main_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, \n",
    "                                                            T_max=num_epochs-warmup_epochs,\n",
    "                                                            eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_params({\n",
    "    \"batch_size\": batch_size,\n",
    "    \"accumulation_epochs\": accumulation_steps,\n",
    "    \"warmup_epochs\": warmup_epochs,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"patience\": patience,\n",
    "    \"encoder\": \"resnet50\",\n",
    "    \"encoder_weights\": \"imagenet\",\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"main_scheduler\": \"CosineAnnealingLR\",\n",
    "    \"base_lr\": 3e-4,\n",
    "    \"criterion\": \"TverskyFocalLoss\",\n",
    "    \"criterion_params\": {\n",
    "        \"alpha_tversky\": 0.7,\n",
    "        \"beta_tversky\": 0.3,\n",
    "        \"alpha_focal\": 0.7,\n",
    "        \"gamma_focal\": 1.5,\n",
    "        \"ignore_index\": 255\n",
    "    },\n",
    "    \"inria_params\":{\n",
    "        'res': 512,\n",
    "        'overlap': 0.125\n",
    "    }\n",
    "})\n",
    "\n",
    "mlflow.log_text(json.dumps(train_augs.to_dict(), indent=2), 'train_augmentations.json')\n",
    "mlflow.log_text(json.dumps(val_augs.to_dict(), indent=2), 'val_augmentations.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit(parent_id, model, criterion, num_epochs,\n",
    "              optimizer, patience, train_loader, val_loader,\n",
    "              main_scheduler, warmup_scheduler, warmup_epochs,\n",
    "              accumulation_steps, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dec8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tag('status', 'success')\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8196f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e683c5e",
   "metadata": {},
   "source": [
    "# ONNX Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519506b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Unet(encoder_name='resnet50', encoder_weights='imagenet').to(DEVICE)\n",
    "checkpoint = torch.load('artifacts\\\\Unet_ResNet50_try_5\\\\checkpoints\\\\best_epoch_21.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e717c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 384, 384, device=DEVICE)\n",
    "onnx_file_path = '..\\\\models\\\\test\\\\Unet_1.0_fp16.onnx'\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,  \n",
    "        dummy_input,  \n",
    "        onnx_file_path, \n",
    "        export_params=True,\n",
    "        opset_version=20,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\"},\n",
    "            \"output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
